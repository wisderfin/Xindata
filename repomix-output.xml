This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  data/
    freelancers.csv
  repository/
    CLI/
      __init__.py
      CLI.py
    llm/
      __init__.py
      llm.py
      prompts.py
    query/
      __init__.py
      query.py
  __main__.py
  settings.py
.env.example
.gitignore
docker-compose.yaml
Dockerfile
makefile
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.



<file path="app/repository/CLI/__init__.py">
from repository.CLI.CLI import CLIRepository

cli_repository = CLIRepository()
</file>

<file path="README.md">
# Xindata
</file>

<file path="app/repository/llm/prompts.py">
from enum import Enum


class Prompts(str, Enum):
    data_analyst = "You are a specialist in data analysis. Answer clearly, briefly and to the point, and only according to the information provided below."
</file>

<file path="app/repository/query/__init__.py">
from repository.query.query import QueryRepository


query_repository = QueryRepository()
</file>

<file path="app/repository/query/query.py">
import pandas as pd
from repository.llm.llm import LLMRepository
import json


class QueryRepository:
    def __init__(self):
        self.path: str = "/app/app/data/freelancers.csv"
        self.data: pd.DataFrame = pd.read_csv(self.path)

    def get(self, question: str) -> str:
        prompt = f"Give me the names of the columns you need to answer the next question. Just give me the names of the columns separated by commas. QUESTIONS: {question}"
        context = str(self.data.columns.tolist())
        llm = LLMRepository()
        columns = llm.ask(question=prompt, context=context).split(", ")
        return self.concentrate(columns)

    def concentrate(self, columns: list[str]) -> str:
        valid_columns = [col for col in columns if col in self.data.columns]
        filtered_data = self.data[valid_columns]
        records = filtered_data.to_dict(orient="records")

        return json.dumps(records, indent=0)
</file>

<file path="Dockerfile">
FROM python:3.11-slim

WORKDIR /app

RUN pip install uv

COPY pyproject.toml .
COPY app/ ./app/
COPY .env ./app/.env

RUN uv venv
RUN uv pip install --no-cache-dir .

ENV PATH="/app/.venv/bin:$PATH"
</file>

<file path="pyproject.toml">
[project]
name = "xindata"
version = "0.1.0"
description = "test for xindata inc."
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "pydantic-settings",
    "typer",
    "pandas",
    "ollama"
]
</file>

<file path="app/__main__.py">
from repository.CLI import cli_repository as cli

if __name__ == "__main__":
    cli.run()
</file>

<file path="app/settings.py">
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    LLM_MODEL: str

    OLLAMA_HOST: str
    OLLAMA_PORT: int

    @property
    def OLLAMA_URL(self) -> str:
        return f'http://{self.OLLAMA_HOST}:{self.OLLAMA_PORT}'

    class Config:
        env_file = 'app/.env'
        extra = 'allow'

settings = Settings()
</file>

<file path=".env.example">
# llm and ollama
LLM_MODEL=gemma3:4b  # model from ollama.com

OLLAMA_HOST=ollama
OLLAMA_PORT=11434
</file>

<file path="makefile">
include .env
export $(shell sed -E '/^\s*#/d;/^\s*$$/d;s/=.*//' .env) # import all variables from .env

first-run:
	docker-compose up --build -d --remove-orphans
	docker-compose exec ollama ollama pull ${LLM_MODEL}
	docker-compose run app sh -c "uv run app"

run:
	docker-compose up -d
	docker-compose run app sh -c "uv run app"
</file>

<file path="app/repository/CLI/CLI.py">
import typer

from repository.query import query_repository as query
from repository.llm.llm import LLMRepository

class CLIRepository:
    def run(self) -> None:
        while True:
            question = typer.prompt("Введите вопрос (или 'exit' для выхода)")
            question = question.encode('utf-8', 'ignore').decode('utf-8')
            if question.lower() == "exit":
                break
            context = str(query.get(question))
            typer.echo(f'---------- {context}')
            llm = LLMRepository()
            response = llm.ask(question, context)
            typer.echo(response)
</file>

<file path="app/repository/llm/__init__.py">
from repository.llm.llm import LLMRepository

llm_repository = LLMRepository()
</file>

<file path="app/repository/llm/llm.py">
import ollama

from repository.llm.prompts import Prompts
from settings import settings


class LLMRepository:
    def __init__(self):
        self.role_prompt = Prompts.data_analyst.value
        self.client = ollama.Client(host=settings.OLLAMA_URL)

    def ask(self, question: str, context: str) -> str:
        prompt = f'Role: {self.role_prompt}\n\nPrompt: {question}\n\nContext: {context}'

        response = self.client.chat(
            model=settings.LLM_MODEL, messages=[{'role': 'user', 'content': prompt}],
        )
        return response["message"]["content"].strip()
</file>

<file path="docker-compose.yaml">
services:
  ollama:
    image: ollama/ollama
    container_name: ${OLLAMA_HOST}
    ports:
      - "${OLLAMA_PORT}:${OLLAMA_PORT}"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - global
    restart: unless-stopped

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: app
    networks:
      - global

networks:
  global:
    driver: bridge

volumes:
  ollama_data:
</file>

<file path=".gitignore">
# virtual environment
.venv
.python-version
build/
*.egg-info
uv.lock

# cache
__pycache__

# environment variables
.env
</file>

</files>
